{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b064c456-f133-42bf-8534-b36e5e59f1fb",
   "metadata": {},
   "source": [
    "### 2.2.2 对称动态量化代码实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085a3708-6a12-4ef9-a3d4-fd6fc879f6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原参数: tensor([[ 0.1234,  0.5678,  0.9012],\n",
      "        [-0.2468, -0.1357,  0.3579]])\n",
      "量化后(int8): tensor([[ 17,  80, 127],\n",
      "        [-35, -19,  51]], dtype=torch.int8)\n",
      "反量化后: tensor([[ 0.1202,  0.5655,  0.8977],\n",
      "        [-0.2474, -0.1343,  0.3605]])\n",
      "量化参数 - scale: 0.007068, zero_point: 0\n",
      "\n",
      "浮点模型输出: tensor([[0.3380, 0.7414]], grad_fn=<MmBackward0>)\n",
      "量化模型输出: tensor([[0.3537, 0.7434]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建简单模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 2, bias=False)\n",
    "        # 设置固定权重便于观察\n",
    "        self.linear.weight.data = torch.tensor(\n",
    "            [[0.1234, 0.5678, 0.9012],\n",
    "             [-0.2468, -0.1357, 0.3579]],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleModel()\n",
    "\n",
    "# 获取第一个权重的原始参数\n",
    "original_param = model.linear.weight.data\n",
    "print(f\"原参数: {original_param}\")\n",
    "\n",
    "# 动态量化\n",
    "# torch.ao.quantization.quantize_dynamic 参数详解:\n",
    "# 1. model: 要进行量化的模型\n",
    "# 2. {nn.Linear}: 指定要量化的模块类型集合(这里只量化Linear层)\n",
    "# 3. dtype: 量化数据类型(torch.qint8表示8位有符号整数)\n",
    "# 返回值: 量化后的模型(只有指定模块被量化，其他保持不变)\n",
    "model_int8 = torch.ao.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {nn.Linear},\n",
    "    dtype=torch.qint8 # 8位有符号整数（默认）\n",
    ")\n",
    "\n",
    "# 获取量化参数\n",
    "# 重要: 量化后，model_int8.linear.weight 不再是一个属性，而是一个函数!\n",
    "# 调用 weight() 返回 PackedParams 对象\n",
    "quantized_weight_all = model_int8.linear.weight()\n",
    "\n",
    "# 正确获取参数的方法:\n",
    "# 1. 获取反量化后的浮点参数(用于计算)\n",
    "dequantized_param = quantized_weight_all.dequantize()\n",
    "\n",
    "# 2. 获取int8表示(实际存储的量化值)\n",
    "int8_param = quantized_weight_all.int_repr()\n",
    "\n",
    "# 3. 获取量化参数\n",
    "# q_scale: 量化缩放因子，用于将浮点数映射到整数范围\n",
    "# q_zero_point: 零点偏移，表示浮点0对应的整数值\n",
    "print(f\"量化后(int8): {int8_param}\")\n",
    "print(f\"反量化后: {dequantized_param}\")\n",
    "print(f\"量化参数 - scale: {quantized_weight_all.q_scale():.6f}, zero_point: {quantized_weight_all.q_zero_point()}\")\n",
    "\n",
    "# 测试推理\n",
    "test_input = torch.randn(1, 3)\n",
    "output_fp32 = model(test_input)\n",
    "output_int8 = model_int8(test_input)\n",
    "print(f\"\\n浮点模型输出: {output_fp32}\")\n",
    "print(f\"量化模型输出: {output_int8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4585e-c695-4044-bdb6-6f1369481f75",
   "metadata": {},
   "source": [
    "### 3.2.2 对称静态量化代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf926ee4-be78-49b1-815b-c940916f4960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浮点模型输出:\n",
      "tensor([[0.1677, 0.2300]], grad_fn=<MmBackward0>)\n",
      "\n",
      "============================================================\n",
      "linear1 原始FP32权重:\n",
      "tensor([[ 0.1234,  0.5678,  0.9012],\n",
      "        [-0.2468, -0.1357,  0.3579],\n",
      "        [ 0.4680,  0.7890, -0.1011],\n",
      "        [-0.3234,  0.6543, -0.9876]])\n",
      "\n",
      "linear1 按通道量化INT8权重:\n",
      "tensor([[  17,   80,  127],\n",
      "        [ -88,  -48,  127],\n",
      "        [  76,  127,  -16],\n",
      "        [ -42,   84, -128]], dtype=torch.int8)\n",
      "\n",
      "linear1 反量化FP32权重:\n",
      "tensor([[ 0.1202,  0.5655,  0.8977],\n",
      "        [-0.2470, -0.1347,  0.3565],\n",
      "        [ 0.4703,  0.7859, -0.0990],\n",
      "        [-0.3253,  0.6507, -0.9915]])\n",
      "\n",
      "linear1 按通道量化参数（4个通道）:\n",
      "  通道0 - scale: 0.007068, zero_point: 0\n",
      "  通道1 - scale: 0.002807, zero_point: 0\n",
      "  通道2 - scale: 0.006188, zero_point: 0\n",
      "  通道3 - scale: 0.007746, zero_point: 0\n",
      "  通道维度: axis=0（0=输出特征维度）\n",
      "\n",
      "激活量化参数（按张量） - scale: 0.030678, zero_point: 55\n",
      "  说明：激活无多通道独立分布特征，按张量量化（全局单参数）效率更高，精度满足需求（区别于权重的按通道量化）\n",
      "============================================================\n",
      "\n",
      "按通道量化模型输出:\n",
      "tensor([[0.1640, 0.2343]])\n",
      "\n",
      "量化误差（L2范数）: 0.005609\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.ao import quantization\n",
    "\n",
    "# 1. 定义待量化模型（含量化/反量化节点）\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = quantization.QuantStub()  # 输入FP32→INT8\n",
    "        self.dequant = quantization.DeQuantStub()  # 输出INT8→FP32\n",
    "        \n",
    "        self.linear1 = nn.Linear(3, 4, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4, 2, bias=False)\n",
    "        \n",
    "        # 固定权重，便于观察量化效果\n",
    "        self.linear1.weight.data = torch.tensor(\n",
    "            [[0.1234, 0.5678, 0.9012],\n",
    "             [-0.2468, -0.1357, 0.3579],\n",
    "             [0.4680, 0.7890, -0.1011],\n",
    "             [-0.3234, 0.6543, -0.9876]],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.linear2.weight.data = torch.tensor(\n",
    "            [[0.1122, -0.3344, 0.5566, -0.7788],\n",
    "             [0.2233, -0.4455, 0.6677, -0.8899]],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# 2. 初始化浮点模型，获取基准输出\n",
    "model_fp32 = SimpleModel()\n",
    "model_fp32.eval()  # 量化仅支持eval模式\n",
    "torch.manual_seed(42)  # 固定种子，结果可复现\n",
    "test_input = torch.randn(1, 3)\n",
    "output_fp32 = model_fp32(test_input)\n",
    "print(f\"浮点模型输出:\\n{output_fp32}\\n\")\n",
    "\n",
    "# 3. 量化核心步骤（权重按通道，激活按张量）\n",
    "# 3.1 配置量化规则：fbgemm默认权重按通道量化，激活按张量量化\n",
    "qconfig = quantization.get_default_qconfig(\"fbgemm\")\n",
    "model_fp32.qconfig = qconfig\n",
    "\n",
    "# 3.2 准备校准：插入假量化节点，开启统计\n",
    "model_prepared = quantization.prepare(model_fp32)\n",
    "\n",
    "# 3.3 激活校准：统计激活分布（模拟10个样本）\n",
    "calibration_data = [torch.randn(1, 3) for _ in range(10)]\n",
    "with torch.no_grad():\n",
    "    for data in calibration_data:\n",
    "        model_prepared(data)\n",
    "\n",
    "# 3.4 转换为INT8模型：权重按通道量化，激活按张量量化\n",
    "model_int8 = quantization.convert(model_prepared)\n",
    "model_int8.eval()\n",
    "\n",
    "# 4. 量化参数分析\n",
    "print(\"=\"*60)\n",
    "# linear1权重（按通道量化：每个输出通道独立scale/zero_point）\n",
    "quantized_linear1 = model_int8.linear1\n",
    "quantized_weight1 = quantized_linear1.weight()\n",
    "\n",
    "print(f\"linear1 原始FP32权重:\\n{model_fp32.linear1.weight.data}\")\n",
    "print(f\"\\nlinear1 按通道量化INT8权重:\\n{quantized_weight1.int_repr()}\")\n",
    "print(f\"\\nlinear1 反量化FP32权重:\\n{quantized_weight1.dequantize()}\")\n",
    "\n",
    "# 按通道量化参数\n",
    "scales = quantized_weight1.q_per_channel_scales()\n",
    "zero_points = quantized_weight1.q_per_channel_zero_points()\n",
    "axis = quantized_weight1.q_per_channel_axis()\n",
    "print(f\"\\nlinear1 按通道量化参数（{len(scales)}个通道）:\")\n",
    "for i in range(len(scales)):\n",
    "    print(f\"  通道{i} - scale: {scales[i].item():.6f}, zero_point: {zero_points[i].item()}\")\n",
    "print(f\"  通道维度: axis={axis}（0=输出特征维度）\")\n",
    "\n",
    "# 激活量化参数（按张量量化：全激活张量共用1套scale/zero_point，无多通道独立分布特征，效率更高且精度足够）\n",
    "quant_stub = model_int8.quant\n",
    "act_scale = quant_stub.scale.item()\n",
    "act_zero_point = quant_stub.zero_point.item()\n",
    "print(f\"\\n激活量化参数（按张量） - scale: {act_scale:.6f}, zero_point: {act_zero_point}\")\n",
    "print(\"  说明：激活无多通道独立分布特征，按张量量化（全局单参数）效率更高，精度满足需求（区别于权重的按通道量化）\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 5. 推理与误差对比\n",
    "output_int8 = model_int8(test_input)\n",
    "print(f\"\\n按通道量化模型输出:\\n{output_int8}\")\n",
    "quant_error = torch.norm(output_fp32 - output_int8).item()\n",
    "print(f\"\\n量化误差（L2范数）: {quant_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235917fd-9ad4-4b82-8205-88c8a1b47cfb",
   "metadata": {},
   "source": [
    "### 量化感知训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ab315d-b0fa-41e8-8d97-4f674555807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始量化感知训练（QAT）...\n",
      "Epoch 1, 平均损失: 0.537143\n",
      "Epoch 2, 平均损失: 0.536015\n",
      "Epoch 3, 平均损失: 0.536302\n",
      "\n",
      "============================================================\n",
      "linear1 按通道量化INT8权重:\n",
      "tensor([[  18,   80,  127],\n",
      "        [ -86,  -48,  127],\n",
      "        [  76,  127,  -18],\n",
      "        [ -42,   84, -128]], dtype=torch.int8)\n",
      "\n",
      "linear1 按通道量化参数（4个通道）:\n",
      "  通道0 - scale: 0.007069, zero_point: 0\n",
      "  通道1 - scale: 0.002813, zero_point: 0\n",
      "  通道2 - scale: 0.006188, zero_point: 0\n",
      "  通道3 - scale: 0.007746, zero_point: 0\n",
      "\n",
      "激活量化参数（按张量） - scale: 0.003848, zero_point: 0\n",
      "============================================================\n",
      "\n",
      "训练前浮点模型输出:\n",
      "tensor([[-0.1679, -0.1966]], grad_fn=<MmBackward0>)\n",
      "QAT量化模型输出:\n",
      "tensor([[0.0000, 0.0279]])\n",
      "QAT量化误差（L2范数）: 0.280334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.ao import quantization\n",
    "\n",
    "# 1. 定义待量化模型（含量化/反量化节点）\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = quantization.QuantStub()  # 输入FP32→INT8\n",
    "        self.dequant = quantization.DeQuantStub()  # 输出INT8→FP32\n",
    "        \n",
    "        self.linear1 = nn.Linear(3, 4, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4, 2, bias=False)\n",
    "        \n",
    "        # 固定初始权重，便于观察QAT效果\n",
    "        self.linear1.weight.data = torch.tensor(\n",
    "            [[0.1234, 0.5678, 0.9012],\n",
    "             [-0.2468, -0.1357, 0.3579],\n",
    "             [0.4680, 0.7890, -0.1011],\n",
    "             [-0.3234, 0.6543, -0.9876]],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.linear2.weight.data = torch.tensor(\n",
    "            [[0.1122, -0.3344, 0.5566, -0.7788],\n",
    "             [0.2233, -0.4455, 0.6677, -0.8899]],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# 2. 初始化模型 + 配置QAT量化规则\n",
    "model = SimpleModel()\n",
    "# QAT专用qconfig：训练时模拟量化误差，fbgemm适配x86，权重按通道、激活按张量\n",
    "qconfig = quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "model.qconfig = qconfig\n",
    "\n",
    "# 3. 准备量化感知训练（QAT）模型（核心：插入量化模拟节点，训练时感知量化误差）\n",
    "model_prepared = quantization.prepare_qat(model)\n",
    "model_prepared.train()  # QAT需要在训练模式下执行\n",
    "\n",
    "# 4. 简单的训练循环（新手演示用，仅3轮训练）\n",
    "optimizer = torch.optim.SGD(model_prepared.parameters(), lr=0.01)  # 优化器\n",
    "loss_fn = nn.MSELoss()  # 损失函数（回归任务）\n",
    "torch.manual_seed(42)   # 固定种子，结果可复现\n",
    "\n",
    "# 模拟训练数据：10个样本，输入3维，标签2维（和模型输出匹配）\n",
    "train_data = [torch.randn(1, 3) for _ in range(10)]\n",
    "train_labels = [torch.randn(1, 2) for _ in range(10)]\n",
    "\n",
    "# 训练循环（QAT核心：训练过程中让模型适应量化误差）\n",
    "print(\"开始量化感知训练（QAT）...\")\n",
    "for epoch in range(3):\n",
    "    total_loss = 0.0\n",
    "    for x, y in zip(train_data, train_labels):\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        output = model_prepared(x)  # 前向传播（含量化模拟）\n",
    "        loss = loss_fn(output, y)   # 计算损失\n",
    "        loss.backward()             # 反向传播\n",
    "        optimizer.step()            # 更新权重\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, 平均损失: {total_loss/len(train_data):.6f}\")\n",
    "\n",
    "# 5. 训练完成后，转换为真正的INT8量化模型\n",
    "model_prepared.eval()  # 转换前切回推理模式\n",
    "model_int8 = quantization.convert(model_prepared)\n",
    "\n",
    "# 6. 测试量化模型效果\n",
    "test_input = torch.randn(1, 3)\n",
    "# 原始浮点模型（训练前）输出对比\n",
    "model.eval()\n",
    "output_fp32 = model(test_input)\n",
    "# QAT量化模型输出\n",
    "output_int8 = model_int8(test_input)\n",
    "\n",
    "# 7. 量化参数分析\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "# linear1权重（按通道量化）\n",
    "quantized_linear1 = model_int8.linear1\n",
    "quantized_weight1 = quantized_linear1.weight()\n",
    "\n",
    "print(f\"linear1 按通道量化INT8权重:\\n{quantized_weight1.int_repr()}\")\n",
    "# 按通道量化参数\n",
    "scales = quantized_weight1.q_per_channel_scales()\n",
    "zero_points = quantized_weight1.q_per_channel_zero_points()\n",
    "print(f\"\\nlinear1 按通道量化参数（{len(scales)}个通道）:\")\n",
    "for i in range(len(scales)):\n",
    "    print(f\"  通道{i} - scale: {scales[i].item():.6f}, zero_point: {zero_points[i].item()}\")\n",
    "\n",
    "# 激活量化参数（按张量：全激活张量共用1套scale/zero_point，无多通道独立分布特征，效率更高且精度足够）\n",
    "quant_stub = model_int8.quant\n",
    "act_scale = quant_stub.scale.item()\n",
    "act_zero_point = quant_stub.zero_point.item()\n",
    "print(f\"\\n激活量化参数（按张量） - scale: {act_scale:.6f}, zero_point: {act_zero_point}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 8. 结果对比\n",
    "print(f\"\\n训练前浮点模型输出:\\n{output_fp32}\")\n",
    "print(f\"QAT量化模型输出:\\n{output_int8}\")\n",
    "# 计算量化误差\n",
    "quant_error = torch.norm(output_fp32 - output_int8).item()\n",
    "print(f\"QAT量化误差（L2范数）: {quant_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc8a06-c372-44e1-afcc-00d7037c3ccb",
   "metadata": {},
   "source": [
    "### LLM.int8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7b2a63-06d0-4107-8094-4a0a0163465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Could not find the bitsandbytes CUDA binary at WindowsPath('D:/Anaconda/envs/deeplearning/lib/site-packages/bitsandbytes/libbitsandbytes_cuda126.dll')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BitsAndBytesConfig' from 'bitsandbytes.optim' (D:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\bitsandbytes\\optim\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 环境检查（修正CUDA版本查询，适配你的配置）\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===== 环境检查 =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BitsAndBytesConfig' from 'bitsandbytes.optim' (D:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\bitsandbytes\\optim\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# 1. 导入核心库 + 环境检查（适配你的conda+cu126+RTX4070）\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from bitsandbytes.optim import BitsAndBytesConfig\n",
    "\n",
    "# 环境检查（修正CUDA版本查询，适配你的配置）\n",
    "print(\"===== 环境检查 =====\")\n",
    "print(f\"Torch版本：{torch.__version__}\")\n",
    "print(f\"CUDA是否可用：{torch.cuda.is_available()}\")\n",
    "print(f\"PyTorch编译的CUDA版本：{torch.version.cuda}\")  # 修正你的错误写法\n",
    "print(f\"可用GPU数量：{torch.cuda.device_count()}\")\n",
    "print(f\"GPU型号：{torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Transformers版本：{__import__('transformers').__version__}\")  # 验证已装的4.52.1\n",
    "\n",
    "# 强制指定RTX4070 GPU\n",
    "device = \"cuda:0\"\n",
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"❌ 检测到GPU但CUDA不可用，请确认显卡驱动/conda环境激活\")\n",
    "\n",
    "# 2. 配置LLM.int8量化参数（兼容transformers 4.52.1）\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 开启LLM.int8量化核心\n",
    "    load_in_8bit_device_map={\"\": 0},  # 强制加载到RTX4070\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # RTX4070加速\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    llm_int8_threshold=6.0  # LLM.int8核心阈值（适配移动端GPU）\n",
    ")\n",
    "\n",
    "# 3. 轻量级模型（RTX4070无压力）\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# 4. 加载8bit量化模型（适配transformers 4.52.1）\n",
    "print(\"\\n===== 加载模型 =====\")\n",
    "print(\"加载8bit模型到RTX4070...（首次下载≈1.1GB，耐心等待）\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device,  # 强制GPU加载\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True  # 减少conda环境的CPU内存占用\n",
    ")\n",
    "\n",
    "# 验证模型状态\n",
    "print(f\"\\n===== 模型验证 =====\")\n",
    "print(f\"模型设备：{next(model.parameters()).device}\")  # 应输出cuda:0\n",
    "print(f\"是否8bit量化：{model.is_loaded_in_8bit}\")  # 应输出True\n",
    "print(f\"GPU显存占用：{torch.cuda.memory_allocated(0)/1024/1024:.0f} MB\")\n",
    "\n",
    "# 5. 加载分词器（适配transformers 4.52.1）\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 修复padding警告\n",
    "\n",
    "# 6. 文本生成测试（RTX4070加速）\n",
    "print(\"\\n===== 生成测试 =====\")\n",
    "prompt = \"请用一句话介绍人工智能\"\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "# 生成参数（兼容transformers 4.52.1）\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=80,\n",
    "    temperature=0.6,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# 输出结果\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"输入：{prompt}\")\n",
    "print(f\"输出：{response}\")\n",
    "\n",
    "# 清理显存（conda环境可选）\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87293a40-7d41-4325-ae3c-51e40973a556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
